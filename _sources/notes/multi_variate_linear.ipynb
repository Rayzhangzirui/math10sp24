{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Multi-variable Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $x_i\\in\\mathbb{R}$, $y_i\\in\\mathbb{R}$, $i= 1,2,..., N$, we want to find the linear function $$y\\approx f(x)=wx +b$$ that fits the relations between $x_i$ and $y_i$. So that given any new $x^{test}$ in the **test** dataset, we can make the prediction $$y^{pred} = w x^{test}+b$$\n",
    "\n",
    "### Training the model\n",
    "\n",
    "- With the training dataset, define the loss function $L(w,b)$ of parameter $w$ and $b$, which is also called **mean squared error** (MSE) \n",
    "\n",
    "$$L(w,b)=\\frac{1}{N}\\sum_{i=1}^N\\big(\\hat{y}^{(i)}-y_i\\big)^2=\\frac{1}{N}\\sum_{i=1}^N\\big((wx_i+b)-y_i\\big)^2,$$\n",
    "\n",
    "where $\\hat{y}^{(i)}$ denotes the predicted value of y at $x_i$, i.e. $\\hat{y}^{(i)} = wx_i+b$.\n",
    "\n",
    "\n",
    "- Then find the minimum of loss function -- note that this is the quadratic function of $w$ and $b$, and we can analytically solve $\\partial_{w}L = \\partial_{b}L =0$, and yields\n",
    "\n",
    "$$w^* = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)}$$\n",
    "\n",
    "\n",
    "$$b^* = \\bar{y}  - w^*\\bar{x}$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of $x$ and of $y$, and $\\text{Cov}(X,Y)$ denotes the estimated covariance (or called sample covariance) between $X$ and $Y$, $\\text{Var}(Y)$ denotes the sample variance of $Y$.\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "- MSE: The smaller MSE indicates better performance\n",
    "- R-Squared: The larger $R^{2}$ (closer to 1) indicates better performance. Compared with MSE, R-squared is **dimensionless**, not dependent on the units of variable. \n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\sum_{i=1}^{N}(y_i-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y_i-\\bar{y})^{2}} = 1 - \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}^{(i)})^{2}}{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\bar{y})^{2}} = 1 - \\frac{\\text{MSE}}{\\text{Var}(Y)}$$\n",
    "\n",
    "Intuitively, MSE is the \"unexplained\" variance, and $R^{2}$ is the proportion of the variance that is explained by the model: if we are not using any model, then our best prediction is the mean of $y$, and MSE is the variance of $y$, and $R^{2}$ is 0; \n",
    "If our model is perfect, then MSE is 0, and $R^{2}$ is 1.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
