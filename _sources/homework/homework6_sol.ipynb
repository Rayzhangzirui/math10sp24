{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6 (Due 5/24)\n",
    "\n",
    "## Name:\n",
    "\n",
    "## ID:\n",
    "\n",
    "## Instructions\n",
    "Run everything (select cell in the menu, and click Run all), export as pdf, and submit the pdf to gradescope. \n",
    "\n",
    "To export as pdf, you can use the following methods: (1) File -> download as -> pdf (2) print as pdf from browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**\n",
    "\n",
    "In this problem, we show that ridge regression can reduce the variance of the model using a synthetic dataset. This problem is similar to this [example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py).\n",
    "\n",
    "(1) Suppose our dataset is generated by the following model:\n",
    "\n",
    "$$ y = 1 + x + \\epsilon $$\n",
    "\n",
    "Generate 1000 samples of $x$ from a uniform distribution between 0 and 1. Generate $\\epsilon$ from a normal distribution with mean 0 and standard deviation 1. Generate $y$ using the above model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "N = 1000\n",
    "np.random.seed(0)  # For reproducibility\n",
    "x = np.random.uniform(0, 1, N)\n",
    "epsilon = np.random.normal(0, 1, N)\n",
    "y = 1 + x + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(2) We can imagine the previous dataset as the whole population. In practice, we can only observe some samples of the population. \n",
    "\n",
    "Repeat the following experiment 100 times:\n",
    "\n",
    "i. Randomly choose 3 samples from the dataset (we can use ``np.random.choice``)\n",
    "\n",
    "ii. Fit a linear regression model and collect the slop and intercept of the model. \n",
    "\n",
    "iii. Fit a ridge regression model with $\\alpha=0.01$ and collect the slope and intercept of the model.\n",
    "\n",
    "\n",
    "After this experiment, we have 100 slopes and 100 intercepts from the linear regression models and 100 slopes and 100 intercepts from the ridge regression models.\n",
    "\n",
    "Compute and compare the mean and standard deviation of the slopes and the intercepts from the linear regression models and the ridge regression models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "Slope: 1.44 +/- 4.58\n",
      "Intercept: 0.77 +/- 2.80\n",
      "Ridge Regression\n",
      "Slope: 1.01 +/- 2.76\n",
      "Intercept: 1.05 +/- 1.64\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "ridge = Ridge(alpha=0.01)\n",
    "\n",
    "# Step 2: Repeat experiment 100 times\n",
    "lm_slopes = []\n",
    "lm_intercepts = []\n",
    "\n",
    "ridge_slopes = []\n",
    "ridge_intercepts = []\n",
    "\n",
    "for _ in range(100):\n",
    "    # Randomly select 2 samples\n",
    "    idx = np.random.choice(range(1000), size=3, replace=False)\n",
    "    x_samples = x[idx].reshape(-1, 1)  # Reshape for sklearn\n",
    "    y_samples = y[idx]\n",
    "\n",
    "    # Fit model\n",
    "    lm.fit(x_samples, y_samples)\n",
    "    ridge.fit(x_samples, y_samples)\n",
    "    \n",
    "    # Collect slope and intercept\n",
    "    lm_slopes.append(lm.coef_[0])\n",
    "    lm_intercepts.append(lm.intercept_)\n",
    "\n",
    "    # Collect slope and intercept for Ridge\n",
    "    ridge_slopes.append(ridge.coef_[0])\n",
    "    ridge_intercepts.append(ridge.intercept_)\n",
    "\n",
    "# Step 3: Compute mean and standard deviation\n",
    "mean_slope = np.mean(lm_slopes)\n",
    "std_slope = np.std(lm_slopes)\n",
    "\n",
    "mean_intercept = np.mean(lm_intercepts)\n",
    "std_intercept = np.std(lm_intercepts)\n",
    "\n",
    "mean_slope_ridge = np.mean(ridge_slopes)\n",
    "std_slope_ridge = np.std(ridge_slopes)\n",
    "\n",
    "mean_intercept_ridge = np.mean(ridge_intercepts)\n",
    "std_intercept_ridge = np.std(ridge_intercepts)\n",
    "\n",
    "print(\"Linear Regression\")\n",
    "print(f\"Slope: {mean_slope:.2f} +/- {std_slope:.2f}\")\n",
    "print(f\"Intercept: {mean_intercept:.2f} +/- {std_intercept:.2f}\")\n",
    "\n",
    "print(\"Ridge Regression\")\n",
    "print(f\"Slope: {mean_slope_ridge:.2f} +/- {std_slope_ridge:.2f}\")\n",
    "print(f\"Intercept: {mean_intercept_ridge:.2f} +/- {std_intercept_ridge:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2** \n",
    "\n",
    "Sometimes the number of features can be much larger than the number of samples. This is called the high-dimensional dataset.\n",
    "\n",
    "In this problem, we compare Lasso and Ridge regression on a synthetic high-dimensional dataset with n = 20 and p = 100.\n",
    "\n",
    "Each feature vector $X_0$, ... $X_{99}$ is generated from a normal distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "The true model is\n",
    "\n",
    "$$ y = 3X_0 - X_1 + 5 X_2 $$\n",
    "\n",
    "That is, only a small number of features are actually relevant to the target variable $y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT modify this cell\n",
    "# Generate synthetic high-dimensional data\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 20  # number of observations\n",
    "p = 100  # number of features\n",
    "X = np.random.randn(n, p)\n",
    "true_coef = np.concatenate([np.array([3, -2, 5]), np.zeros(p - 3)])\n",
    "y = np.dot(X, true_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a Lasso and a Ridge regression model to this dataset without the intercept term and use $\\alpha=0.1$.\n",
    "\n",
    "(1) Compute and compare the means square error of the two models. \n",
    "\n",
    "(2) Collect and compare the coefficents of the two models. \n",
    "\n",
    "(3) If we say feature $X_i$ is relevant if the coefficient $|\\beta_i| > 0.1$, what are the indices of the relevant features identified by the Lasso model and the Ridge model? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Estimated Coefficients: [ 2.8896224  -1.86204855  4.90596724  0.         -0.         -0.\n",
      " -0.          0.         -0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -0.          0.         -0.          0.\n",
      "  0.         -0.          0.         -0.          0.          0.\n",
      " -0.          0.         -0.          0.         -0.         -0.\n",
      "  0.         -0.          0.         -0.01705282 -0.         -0.\n",
      " -0.          0.          0.         -0.          0.         -0.\n",
      " -0.         -0.         -0.          0.         -0.         -0.\n",
      " -0.          0.          0.          0.         -0.          0.\n",
      " -0.         -0.         -0.         -0.         -0.          0.\n",
      " -0.         -0.         -0.          0.         -0.          0.\n",
      "  0.         -0.          0.         -0.         -0.         -0.\n",
      " -0.          0.          0.         -0.          0.         -0.\n",
      "  0.          0.         -0.          0.         -0.          0.\n",
      " -0.          0.          0.          0.          0.         -0.\n",
      "  0.          0.          0.         -0.        ]\n",
      "Ridge Estimated Coefficients: [ 0.55898585 -0.51818799  1.03911899  0.12101928 -0.03673009 -0.37270107\n",
      " -0.29183208  0.16404584 -0.29500107  0.34890786  0.2531242   0.27975387\n",
      "  0.28731744 -0.24431456  0.24392325  0.24641672  0.58636658  0.27232383\n",
      " -0.05151523  0.18486079 -0.08240379  0.03755489  0.0773111   0.0667142\n",
      "  0.05999731  0.02712416  0.3151059  -0.1358621   0.1719881  -0.00925289\n",
      " -0.2407705   0.40563011 -0.05866761  0.33979682 -0.33727622 -0.34517816\n",
      "  0.3020337  -0.02905779  0.10715614 -0.56150971 -0.0207445  -0.15797826\n",
      " -0.35350836  0.22779835 -0.10588873 -0.16725091  0.21795621 -0.39488467\n",
      " -0.18216364 -0.19378187 -0.21106845  0.30935122 -0.04318175 -0.39535202\n",
      " -0.35873794  0.10861192  0.05882666  0.33847306 -0.08091878  0.18347366\n",
      " -0.19045767 -0.02578437 -0.25766707  0.0200108   0.15646167  0.14643216\n",
      " -0.08523547  0.06156932 -0.2451973   0.12830601  0.1437079   0.15192919\n",
      "  0.38861557 -0.34737339  0.2927217  -0.10003715 -0.04304044 -0.14541525\n",
      " -0.21402675  0.09587481  0.07289354 -0.16409111  0.21538791  0.07747485\n",
      " -0.36212103  0.25886075 -0.3341531   0.60470465 -0.53823008  0.12623877\n",
      " -0.14074303  0.39301617  0.49995867  0.10322566  0.55099693  0.05721581\n",
      "  0.22712459  0.01756174 -0.10027346 -0.17393604]\n",
      "Lasso Relevant Features: [ 0  1  2 39]\n",
      "Ridge Relevant Features: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "Lasso Mean Squared Error: 0.03250277108009788\n",
      "Ridge Mean Squared Error: 5.2235399069247656e-05\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.01\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=0.1, fit_intercept=False)\n",
    "lasso.fit(X, y)\n",
    "lasso_coef = lasso.coef_\n",
    "\n",
    "# find the index of |coefficent| > 0.1\n",
    "lasso_relevant_features = np.where(np.abs(lasso_coef) > threshold)[0]\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=0.1, fit_intercept=False)\n",
    "ridge.fit(X, y)\n",
    "ridge_coef = ridge.coef_\n",
    "\n",
    "# find the index of |coefficent| > 0.1\n",
    "ridge_relevant_features = np.where(np.abs(ridge_coef) > threshold)[0]\n",
    "\n",
    "\n",
    "# Results Comparison\n",
    "# print(\"True Coefficients:\", true_coef)  # First 10 coefficients for brevity\n",
    "print(\"Lasso Estimated Coefficients:\", lasso_coef)\n",
    "print(\"Ridge Estimated Coefficients:\", ridge_coef)\n",
    "\n",
    "print(\"Lasso Relevant Features:\", lasso_relevant_features)\n",
    "print(\"Ridge Relevant Features:\", ridge_relevant_features)\n",
    "\n",
    "print(\"Lasso Mean Squared Error:\", mean_squared_error(y, lasso.predict(X)))\n",
    "print(\"Ridge Mean Squared Error:\", mean_squared_error(y, ridge.predict(X)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
